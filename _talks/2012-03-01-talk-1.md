---
title: "A Story of Coefficients: Researching Emotion Classification through CNNs with a Comparative Analysis of Features (Speech Emotion Recognition)"
collection: 
type: "Constructing A 7 Tube, 20W Valve Amplifier"
permalink: /work/eclassification
venue: null
date: 2021-11-3
location: null
---



In the integration of a comprehensive system which interacts with humans, there begs the question of emotion classification. Check out my [publication](https://ieeexplore.ieee.org/document/9651089)





Speech is a large part of the medium of communication, either through a machine or with another fellow human, 
and I was very intrigued with how well a neural network could 
identify a human’s emotions through speech.

A huge chunk of research in audio signal processing begins with the extraction of MFCCs. MFCCs are a representation of audio similarly to how RGB 
values represent a photo. Rather than colours, MFCCs showcase the short-term power spectrum of a sound. This helps machines process how humans sound, and how someone sounds/perceived by the human ear isn't linear, so the Mel scale approximates the human ear's perception.

There are various factors that affect audio data processing; the quality of the equipment with which the audio was recorded, the amount of noise in the speech, and particularly the background environment (background noises mess with the extraction of coefficients which influences the inference of the model) Using a well-known dataset is key here. 
But using a model trained on a dataset recorded in an optimal environment isn't feasible in more realistic environments.



By researching signal classification, I wanted to create a speech emotion recognition (SER) system, which is robust and ideally suppresses the stationary background noise. The
log-mel frontend, which includes the mel-filterbank energy extraction, is followed by the log transform. However, as the log function utilizes a large part of its dynamic range for lower levels like silence, it isn’t very helpful as these parts of the signal are usually the least informative. 


Per-channel energy normalization (PCEN) was introduced by a few researchers at Google in 2017. Vincent Lostenlen et al. had further investigated the adequacy of [PCENs](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/lostanlen_pcen_spl2018.pdf) for spectrogram-based pattern recognition. By understanding that per-channel energy normalization (PCEN) solves the above issue (by outperforming the log transform of mel-frequency spectrogram) with both adaptive gain control and dynamic range control, I was able to extract values from the input signal using the PCEN procedure and feed them as input values to the CNN. 

<img src='/images/pcen_orig.png'>

Source: [PCENs: Why and How](https:/justinsalamon.com/news/per-channel-energy-normalization-why-and-how)

This experimentation yielded a higher accuracy than values that use the log transform. 
I learned that re-evaluating the mathematical transforms in signal processing techniques, 
while fine-tuning the parameters of a procedure to the temporal properties 
of noise, to be diminished, will readily increase a model’s performance. 