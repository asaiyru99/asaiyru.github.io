---
title: "Seeking Attention: Building the Transformer Architecture from Scratch"
collection: talks
type: "Tutorial"
permalink: /work/transformer
venue: null
date: 2023-11-01
location: null
---


Implementing the 'Attention Is All You Need' paper from scratch and discussing its architecture.


Up until recently, most language modelling and transduction tasks were handled by architectures like RNNs, LSTMs etc. I remember this one
Kaggle tutorial that used RNNs to model dinosaur names. Anyway, Vaswani et al. proposed a model architecture that skips ahead of recurrence and convolution, and instead
uses an attention mechanism. This mechanism teaches the model to 'attend' to particular words/tokens in a sequence as not all sections of the input are equally important. The objective is to essentially 
output a weighted sum of the inputs, where the
weight assigned to a value corresponds to its importance to the output element. 

Like most transduction models, this too follows an encoder-decoder model. Prior to building any of the encoder or decoder blocks, I have to embed the data. The data is originally words of a sentence that gets tokenized and is identified through a token id of its own. 
Each token is then translated into a 512 size vector (or whatever the dimensionality dmodel is; it is 512 in the paper).
I used nn.Embedding for this, and provided the vocabulary size and dmodel as the arguments.

<img src='/images/arch.png'>


The paper explains that since they did not use recurrence, they needed to use a method that embeds the absolute or relative position of each token. 
They chose that the positional encodings to be added to the regular input embeddings. 

The positional encoding is defined by the following formulas:

For even dimensions (2i):
$$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) $$

For odd dimensions (2i+1):
$$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) $$

Where:
- `PE(pos, 2i)` and `PE(pos, 2i+1)` are the positional encodings for the even and odd dimensions, respectively.
- `pos` is the position in the input sequence.
- `i` is the dimension index. Where `i` runs from 0 to 255.
- `d_model` is the dimensionality of the model (embedding size).

I didn't quite understand how to interpret this initially in code. 
But then I visualised a size 512 (dmodel) vector for each token, and figured that I can slice the vector and dedicate the positive positions (0,2,4.. etc) to sin, 
and the odd positions to cos. Each dimension corresponds to a sinusoid for each token. The entire encoded vector starts off being size (seq_length, dmodel), and is increased in dimension for the batch size. 
Finally, this vector is added with the embedded input from above. 

<h1> The Sublayers</h1>

<h2> Sublayer 1 - multi head attention </h2>

The input embeddings, according to the diagram, are given to the Multi-Head Attention Block. 
In the paper, they found it beneficial to project the queries, 
keys and values h times. 
This means that instead of a single attention function, 
they divided dmodel by the value of h (number of heads) and parallely performed the attention
mechanism for each h. 

First, I check if dmodel is divisible by the number of heads as the number of heads should be such that there is no remainder. 
In the paper, they chose 8 heads. I then assign a variable `dk` for the number of values for each token. 

`dk` = `dmodel` / `number of heads` = 512 / 8 = 64

 


Q, K, V is known as query, key and value and are three concepts that solidify the attention mechanism.
There are three different matrices of weights of size (`dmodel`, `dmodel`) that are multiplied with the input embeddings that yield Q, K, V. Each weight matrix projects the input embeddings
into a different space as Q, K, V have different purposes. 

Q helps a token figure out what itâ€™s looking for. 

$$
Q' = Q \times W^Q
$$

Here, `Q'` is the query and `Q` represents the input embeddings.

K helps a token broadcast what information it has available.

$$
K' = K \times W^K
$$



V is what will actually be used (after computing attention) to update the representation of the token.


$$
V' = V \times W^V
$$


Now, I calculate the attention scores.

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$




After the attention scores are found for each head, the values are concatenated. Each head will be of size (`seq_len`, `dk`).


$$
\text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
$$


<h2> Sublayer 2 - feed forward network </h2>

After the first sublayer, a fully connected feed-forward network is applied to each position separately and
identically. What this means is that regardless of a token's relationships
with other tokens, or how it was operated on through the
attention mechanism, the same FFN operation is applied just the same.

The activation function used is ReLu, and the parameters vary with the layer. The input and output layer `dmodel`has a dimensionality of 
512, while the inner layer `dff` has a dimensionality of 2048.

$$
\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
$$

How this would be implemented in code, would be first initialising layers L1 and L2. 

`self.L1 = nn.Linear(dmodel, dff)` 

`self.L2 =  nn.Linear(dff, dmodel)` 

and return the value: `self.L2(self.dropout(torch.relu(self.L1(x))))`

Where `x` represents the output of the first sublayer.



<h1> Encoder and Decoder</h1>


Each encoder has 2 sublayers, one dedicated to multi-head attention mechanism and the other dedicated to a position-wise fully
connected feed forward network that uses ReLu as an activation function between the layers.
To help the vanishing gradient issue, there is a residual connection after
each sublayer followed by a layer of normalization. And so it follows: 

Output of each sublayer = `norm`( `x` + `dropout`(`sublayer`(`x`)))

Where `sublayer`(x) is the function applied for that particular sublayer.


The decoder's input is the target embeddings, which is technically what the
model is supposed to predict. It follows the same architecture as the encoder but there are a few changes to be noted.
Firstly, there is an extra sub-layer dedicated to multi head attention that is modified to perform
on the output of the encoder. This is cross attention and in my eyes, where the magic of the transformer
happens. The output of the encoder provides important context from the input embeddings and
the decoder uses it to attend to so it can generate/predict the appropriate output.


The second change is in the first sub-layer that performs self attention. The model
is forbidden to see any tokens that it hasn't generated yet. What this means is while performing attention and deciding the value of a token,
it may only see tokens it has thus far generated to make the decision. 
Otherwise, the model would be cheating. This is done using a triangular mask.

The output embeddings are offset by one position, so that the decoder can only
predict an output at position i on known outputs for positions that are lesser than i.
Due to the masking and the <start> token, at the first position, the decoder may only see the <start>
token and not any data. Only at the second position will it see the first generated word.

<h1> Projection Layer</h1>

A linear transformation is applied to X, which is the output of the three sub-layers. 
So in my `Transformer` class, I create functions to encode and decode the 
embeddings, and then
create a function for the projection layer. 

This is done essentially to convert the size of the dmodel dimension of the embeddings 
into the vocabulary size, which will be what the model predicts. 

With the projection layer, this ends the transformer architecture.








